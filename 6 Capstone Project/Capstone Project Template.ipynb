{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The objective of this project is to create an ETL pipeline for given data on I94 immigration, global land temperatures and US demographics datasets to form an analytics database on immigration events. A use case for this analytics database is to find immigration patterns to the US, for example statistics on non-resident arrivals to the U.S. by type of visa (business, pleasure, student) or dependencies between country of origin and state demographics of the first intended adress.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Scope\n",
    "The main goal of this project is to create a warehouse in the cloud that will allow the analysts to perform analytical queries or build dashboards to answer some business questions. The main dataset that we will use will include data on immigration to the United States, and supplementary datasets will include data on U.S. city demographics and temperature data.\n",
    "Questions to answer with this data:\n",
    "- Non-resident arrivals to the U.S. by country of residence over a period of time\n",
    "- Non-resident arrivals to the U.S. by type of visa (business, pleasure, student)\n",
    "- Non-resident arrivals to the U.S. by mode of transportation\n",
    "- Arrivals to the U.S. by the climate of the original country\n",
    "- Statistics of visitors: age, gender\n",
    "- Dependencies between country of origin and state demographics of the first intended adress\n",
    "- ...\n",
    "\n",
    "This project will be executed with the help of **Amazon Web Services**: Data will be read from the customers repository using **Apache Spark** and stored in a staging area in **AWS S3 bucket**. Then, it will be loaded to the **Amazon Redshift** warehouse using an **Apache Airflow** pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Describe and Gather Data \n",
    "Following data sets are available:\n",
    "- **I94 Immigration Data:** This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. This is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.\n",
    "- **World Temperature Data:** This dataset came from Kaggle. You can read more about it here.\n",
    "- **U.S. City Demographic Data:** This data comes from OpenSoft. You can read more about it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Immigration Data Sample: For the first impression of the data we will take a look on a data sample provided in the workspace. For data analysis we will load a bigger batch\n",
    "\n",
    "dirImmigration = 'immigration_data_sample.csv'\n",
    "dfImmigration = pd.read_csv(dirImmigration)\n",
    "dfImmigration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port',\n",
       "       'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa',\n",
       "       'count', 'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd',\n",
       "       'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum',\n",
       "       'airline', 'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfImmigration.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Description of the dataset:\n",
    "\n",
    "|Column Name|Description|Data Type|\n",
    "|---|---|---|\n",
    "|_cicid_|Record ID|double|\n",
    "|i94yr|Year of arrival, 4 digits|double|\n",
    "|i94mon|Month of arrival, numeric|double|\n",
    "|i94cit|Citizenship country code|double|\n",
    "|_i94res_|Residence country code|double|\n",
    "|i94port|U.S. port of entry (main gateway ports - all modes and air-only)|string|\n",
    "|_arrdate_|Arrival date in the USA|double|\n",
    "|i94mode|Mode of transportation (1=air, 2=sea, 3=land, 9=not reported)|double|\n",
    "|_i94addr_|First Intended Address in the U.S. (or Address while in the U.S.) (state)|string|\n",
    "|depdate|Departure date from the USA|double|\n",
    "|i94bir|Age of Respondent in Years|double|\n",
    "|i94visa|Visa code (1=business, 2=pleasure, 3=student)|double|\n",
    "|count|Used for summary statistics|double|\n",
    "|dtadfile|Character Date Field - Date added to I-94 Files|string|\n",
    "|visapost|Department of State where where Visa was issued|string|\n",
    "|entdepa|Arrival Flag - admitted or paroled into the U.S.|string|\n",
    "|entdepd|Departure Flag - Departed, lost I-94 or is deceased|string|\n",
    "|matflag| Match flag - Match of arrival and departure records|string|\n",
    "|biryear|Birth year, 4 digits|double|\n",
    "|dtaddto|Character Date Field - Date to which admitted to U.S. (allowed to stay until)|string|\n",
    "|gender|Gender|string|\n",
    "|airline|Airline used to arrive in U.S.|string|\n",
    "|admnum|Admission Number|double|\n",
    "|fltno|Flight number of Airline used to arrive in U.S.|string|\n",
    "|visatype|Class of admission legally admitting the non-immigrant to temporarily stay in U.S.|string|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# World Temperature Data\n",
    "\n",
    "dirTemp = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "dfTemp = pd.read_csv(dirTemp)\n",
    "dfTemp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Description of the dataset:\n",
    "\n",
    "|Column Name|Description|Data Type|\n",
    "|---|---|---|\n",
    "|dt|Date in format YYYY-MM-DD|timestamp|\n",
    "|AverageTemperature|Global average land temperature in Celsius|double|\n",
    "|AverageTemperatureUncertainty|The 95% confidence interval around the average|double|\n",
    "|City|City name|string|\n",
    "|Country|Country name|string|\n",
    "|Latitude|Latitude|string|\n",
    "|Longitude|Longitude|string|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demographic Data\n",
    "\n",
    "dirDemogr = 'us-cities-demographics.csv'\n",
    "dfDemogr = pd.read_csv(dirDemogr,sep=';')\n",
    "dfDemogr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Description of the dataset:\n",
    "\n",
    "|Column Name|Description|Data Type|\n",
    "|---|---|---|\n",
    "|City|City name|string|\n",
    "|State|US state|string|\n",
    "|Median Age|Median of the age of the population|double|\n",
    "|Male Population|Number of males|integer|\n",
    "|Female Population|Number of females|integer|\n",
    "|Total Population|Number of the total population|integer|\n",
    "|Number Veterans|Number of veterans|integer|\n",
    "|Foreign-born||integer|\n",
    "|Average Household Size|Average size of the household|double|\n",
    "|State Code||string|\n",
    "|Race||string|\n",
    "|Count||integer|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "Following data sets are available:\n",
    "- **I94 Immigration Data:** This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. This is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.\n",
    "- **World Temperature Data:** This dataset came from Kaggle. You can read more about it here.\n",
    "- **U.S. City Demographic Data:** This data comes from OpenSoft. You can read more about it here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries needed for data analysis\n",
    "import pandas as pd\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, isnan\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load Configuration Data\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a spark session\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\",\n",
    "                                    \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\").\\\n",
    "    enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### I94 Immigration Data\n",
    "The immigration data can be accessed in a folder with the following path: `../../data/18-83510-I94-Data-2016/`.There's a file for each month of the year. An example file name is `i94_apr16_sub.sas7bdat`. Each file has a three-letter abbreviation for the month name. So a full file path for June would look like this: `../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i94_apr16_sub.sas7bdat',\n",
       " 'i94_sep16_sub.sas7bdat',\n",
       " 'i94_nov16_sub.sas7bdat',\n",
       " 'i94_mar16_sub.sas7bdat',\n",
       " 'i94_jun16_sub.sas7bdat',\n",
       " 'i94_aug16_sub.sas7bdat',\n",
       " 'i94_may16_sub.sas7bdat',\n",
       " 'i94_jan16_sub.sas7bdat',\n",
       " 'i94_oct16_sub.sas7bdat',\n",
       " 'i94_jul16_sub.sas7bdat',\n",
       " 'i94_feb16_sub.sas7bdat',\n",
       " 'i94_dec16_sub.sas7bdat']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all the files in the data repository\n",
    "os.listdir('../../data/18-83510-I94-Data-2016/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>None</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN    None   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U     None   1979.0  10282016   None   None   \n",
       "1      NaN   ...           Y     None   1991.0       D/S      M   None   \n",
       "2  20691.0   ...        None        M   1961.0  09302016      M   None   \n",
       "3  20567.0   ...        None        M   1988.0  09302016   None   None   \n",
       "4  20567.0   ...        None        M   2012.0  09302016   None   None   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0    None  1.897628e+09   None       B2  \n",
       "1    None  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data of one month\n",
    "filenameImm = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "dfImmigration = spark.read.format('com.github.saurfang.sas.spark').load(filenameImm)\n",
    "dfImmigration.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfImmigration.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Description of the dataset:\n",
    "\n",
    "|Column Name|Description|Data Type|\n",
    "|---|---|---|\n",
    "|_**cicid**_|Record ID|double|\n",
    "|**i94yr**|Year of arrival, 4 digits|double|\n",
    "|**i94mon**|Month of arrival, numeric|double|\n",
    "|**i94cit**|Citizenship country code|double|\n",
    "|_**i94res**_|Residence country code|double|\n",
    "|**i94port**|U.S. port of entry (main gateway ports - all modes and air-only)|string|\n",
    "|_**arrdate**_|Arrival date in the USA|double|\n",
    "|**i94mode**|Mode of transportation (1=air, 2=sea, 3=land, 9=not reported)|double|\n",
    "|_**i94addr**_|First Intended Address in the U.S. (or Address while in the U.S.) (state)|string|\n",
    "|**depdate**|Departure date from the USA|double|\n",
    "|**i94bir**|Age of Respondent in Years|double|\n",
    "|**i94visa**|Visa code (1=business, 2=pleasure, 3=student)|double|\n",
    "|count|Used for summary statistics|double|\n",
    "|dtadfile|Character Date Field - Date added to I-94 Files|string|\n",
    "|visapost|Department of State where where Visa was issued|string|\n",
    "|entdepa|Arrival Flag - admitted or paroled into the U.S.|string|\n",
    "|entdepd|Departure Flag - Departed, lost I-94 or is deceased|string|\n",
    "|matflag| Match flag - Match of arrival and departure records|string|\n",
    "|**biryear**|Birth year, 4 digits|double|\n",
    "|**dtaddto**|Character Date Field - Date to which admitted to U.S. (allowed to stay until)|string|\n",
    "|**gender**|Gender|string|\n",
    "|**airline**|Airline used to arrive in U.S.|string|\n",
    "|admnum|Admission Number|double|\n",
    "|fltno|Flight number of Airline used to arrive in U.S.|string|\n",
    "|**visatype**|Class of admission legally admitting the non-immigrant to temporarily stay in U.S.|string|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cols</th>\n",
       "      <th>values</th>\n",
       "      <th>missing_values_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cicid</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i94yr</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i94mon</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i94cit</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i94res</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i94port</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arrdate</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i94mode</td>\n",
       "      <td>239</td>\n",
       "      <td>0.007719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i94addr</td>\n",
       "      <td>152592</td>\n",
       "      <td>4.928184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>depdate</td>\n",
       "      <td>142457</td>\n",
       "      <td>4.600859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i94bir</td>\n",
       "      <td>802</td>\n",
       "      <td>0.025902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i94visa</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>count</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dtadfile</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>visapost</td>\n",
       "      <td>1881250</td>\n",
       "      <td>60.757746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>occup</td>\n",
       "      <td>3088187</td>\n",
       "      <td>99.737559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>entdepa</td>\n",
       "      <td>238</td>\n",
       "      <td>0.007687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>entdepd</td>\n",
       "      <td>138429</td>\n",
       "      <td>4.470769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>entdepu</td>\n",
       "      <td>3095921</td>\n",
       "      <td>99.987340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>matflag</td>\n",
       "      <td>138429</td>\n",
       "      <td>4.470769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>biryear</td>\n",
       "      <td>802</td>\n",
       "      <td>0.025902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>dtaddto</td>\n",
       "      <td>477</td>\n",
       "      <td>0.015405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gender</td>\n",
       "      <td>414269</td>\n",
       "      <td>13.379429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>insnum</td>\n",
       "      <td>2982605</td>\n",
       "      <td>96.327632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>airline</td>\n",
       "      <td>83627</td>\n",
       "      <td>2.700857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>admnum</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>fltno</td>\n",
       "      <td>19549</td>\n",
       "      <td>0.631364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>visatype</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cols   values  missing_values_percent\n",
       "0      cicid        0                0.000000\n",
       "1      i94yr        0                0.000000\n",
       "2     i94mon        0                0.000000\n",
       "3     i94cit        0                0.000000\n",
       "4     i94res        0                0.000000\n",
       "5    i94port        0                0.000000\n",
       "6    arrdate        0                0.000000\n",
       "7    i94mode      239                0.007719\n",
       "8    i94addr   152592                4.928184\n",
       "9    depdate   142457                4.600859\n",
       "10    i94bir      802                0.025902\n",
       "11   i94visa        0                0.000000\n",
       "12     count        0                0.000000\n",
       "13  dtadfile        1                0.000032\n",
       "14  visapost  1881250               60.757746\n",
       "15     occup  3088187               99.737559\n",
       "16   entdepa      238                0.007687\n",
       "17   entdepd   138429                4.470769\n",
       "18   entdepu  3095921               99.987340\n",
       "19   matflag   138429                4.470769\n",
       "20   biryear      802                0.025902\n",
       "21   dtaddto      477                0.015405\n",
       "22    gender   414269               13.379429\n",
       "23    insnum  2982605               96.327632\n",
       "24   airline    83627                2.700857\n",
       "25    admnum        0                0.000000\n",
       "26     fltno    19549                0.631364\n",
       "27  visatype        0                0.000000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count percentage of missing values\n",
    "total = dfImmigration.count()\n",
    "\n",
    "nanCountTemp = dfImmigration.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dfImmigration.columns]).toPandas()\n",
    "nanCount = pd.melt(nanCountTemp, var_name='cols', value_name='values')\n",
    "\n",
    "nanCount['missing_values_percent'] = nanCount['values']/total*100\n",
    "\n",
    "nanCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop columns with missing values > 85%\n",
    "columnsToDrop = ['occup', 'entdepu','insnum']\n",
    "dfImmigration = dfImmigration.drop(*columnsToDrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfImmigration has 0 duplucates.\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate entries\n",
    "countAll = dfImmigration.count()\n",
    "\n",
    "dfImmigrationT = dfImmigration.dropDuplicates(['cicid'])\n",
    "countWithDupl = dfImmigrationT.count()\n",
    "\n",
    "print(\"dfImmigration has \" + str(countAll - countWithDupl) + \" duplucates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfImmigration has 0 NaN-values.\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "dfImmigrationT = dfImmigration.dropna(how='all', subset=['cicid'])\n",
    "countWithNa = dfImmigrationT.count()\n",
    "\n",
    "print(\"dfImmigration has \" + str(countAll - countWithNa) + \" NaN-values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The immigration dataset will provide the fact table which will be at the center of the star schema model of our data warehouse. We discovered the columns that can be removed because of the high amount of missing values. Also some columns won't be used because the information is not necessary for our model. The column *cicid* has no duplucates and NaN-values and can be used as a primary index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### World Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0 1743-11-01               6.068                          1.737  Århus   \n",
       "1 1743-12-01                 NaN                            NaN  Århus   \n",
       "2 1744-01-01                 NaN                            NaN  Århus   \n",
       "3 1744-02-01                 NaN                            NaN  Århus   \n",
       "4 1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTempDir = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "dfTemp = spark.read.csv(dfTempDir, header=True, inferSchema=True)\n",
    "dfTemp.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTemp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Description of the dataset:\n",
    "\n",
    "|Column Name|Description|Data Type|\n",
    "|---|---|---|\n",
    "|dt|Date in format YYYY-MM-DD|timestamp|\n",
    "|AverageTemperature|Global average land temperature in Celsius|double|\n",
    "|AverageTemperatureUncertainty|The 95% confidence interval around the average|double|\n",
    "|City|City name|string|\n",
    "|Country|Country name|string|\n",
    "|Latitude|Latitude|string|\n",
    "|Longitude|Longitude|string|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cols</th>\n",
       "      <th>values</th>\n",
       "      <th>missing_values_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AverageTemperature</td>\n",
       "      <td>364130</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AverageTemperatureUncertainty</td>\n",
       "      <td>364130</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>City</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Country</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Latitude</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Longitude</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            cols  values  missing_values_percent\n",
       "0                             dt       0                     0.0\n",
       "1             AverageTemperature  364130                     0.0\n",
       "2  AverageTemperatureUncertainty  364130                     0.0\n",
       "3                           City       0                     0.0\n",
       "4                        Country       0                     0.0\n",
       "5                       Latitude       0                     0.0\n",
       "6                      Longitude       0                     0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count percentage of missing values\n",
    "total = dfTemp.count()\n",
    "\n",
    "dfTemp = dfTemp.withColumn(\"dt\",col(\"dt\").cast(StringType())) # convert dt column type to string\n",
    "\n",
    "nanCountTemp = dfTemp.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dfTemp.columns]).toPandas()\n",
    "nanCount = pd.melt(nanCountTemp, var_name='cols', value_name='values')\n",
    "\n",
    "nanCount['missing_values_percent'] = nan_count['values']/total*100\n",
    "\n",
    "nanCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfTemp has 46034 duplucates.\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate entries\n",
    "countAll = dfTemp.count()\n",
    "\n",
    "dfTempT = dfTemp.dropDuplicates(subset=['dt', 'City', 'Country'])\n",
    "countWithDupl = dfTempT.count()\n",
    "\n",
    "print(\"dfTemp has \" + str(countAll - countWithDupl) + \" duplucates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfTemp has 364130 NaN-values.\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "dfTempT = dfTemp.dropna(how='all', subset=['AverageTemperature'])\n",
    "countWithNa = dfTempT.count()\n",
    "\n",
    "print(\"dfTemp has \" + str(countAll - countWithNa) + \" NaN-values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since the immigration dataset only has data of the US National Tourism Office in the year of 2016, the  majority of the data here is not necessary for our model. Thereforewe will use an aggregation of the averaged temperature by country over the whole provided period of time. The latitude and the longitude are given for the difference cities therefore we won't use them in the aggregation. We will also generate a look-up table *i94_country* from the file ```I94_SAS_Labels_Descriptions.SAS``` provided in the workspace. In this way we can connect each country with its country code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### U.S. City Demographic Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040</td>\n",
       "      <td>46799</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819</td>\n",
       "      <td>8229</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127</td>\n",
       "      <td>87105</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821</td>\n",
       "      <td>33878</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040</td>\n",
       "      <td>143873</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829</td>\n",
       "      <td>86253</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8            40601   \n",
       "1            Quincy  Massachusetts        41.0            44129   \n",
       "2            Hoover        Alabama        38.5            38040   \n",
       "3  Rancho Cucamonga     California        34.5            88127   \n",
       "4            Newark     New Jersey        34.6           138040   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0              41862             82463                1562         30908   \n",
       "1              49500             93629                4147         32935   \n",
       "2              46799             84839                4819          8229   \n",
       "3              87105            175232                5821         33878   \n",
       "4             143873            281913                5829         86253   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfDemogrDir = 'us-cities-demographics.csv'\n",
    "dfDemogr = spark.read.csv(dfDemogrDir, inferSchema=True, header=True, sep=';')\n",
    "dfDemogr.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDemogr.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Description of the dataset:\n",
    "\n",
    "|Column Name|Description|Data Type|\n",
    "|---|---|---|\n",
    "|City|City name|string|\n",
    "|State|US state|string|\n",
    "|Median Age|Median of the age of the population|double|\n",
    "|Male Population|Number of males|integer|\n",
    "|Female Population|Number of females|integer|\n",
    "|Total Population|Number of the total population|integer|\n",
    "|Number Veterans|Number of veterans|integer|\n",
    "|Foreign-born||integer|\n",
    "|Average Household Size|Average size of the household|double|\n",
    "|State Code||string|\n",
    "|Race||string|\n",
    "|Count||integer|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cols</th>\n",
       "      <th>values</th>\n",
       "      <th>missing_values_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>City</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>State</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Median Age</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male Population</td>\n",
       "      <td>3</td>\n",
       "      <td>0.103770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female Population</td>\n",
       "      <td>3</td>\n",
       "      <td>0.103770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Total Population</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Veterans</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Foreign-born</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Average Household Size</td>\n",
       "      <td>16</td>\n",
       "      <td>0.553442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>State Code</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Race</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Count</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      cols  values  missing_values_percent\n",
       "0                     City       0                0.000000\n",
       "1                    State       0                0.000000\n",
       "2               Median Age       0                0.000000\n",
       "3          Male Population       3                0.103770\n",
       "4        Female Population       3                0.103770\n",
       "5         Total Population       0                0.000000\n",
       "6       Number of Veterans      13                0.449671\n",
       "7             Foreign-born      13                0.449671\n",
       "8   Average Household Size      16                0.553442\n",
       "9               State Code       0                0.000000\n",
       "10                    Race       0                0.000000\n",
       "11                   Count       0                0.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count percentage of missing values\n",
    "total = dfDemogr.count()\n",
    "\n",
    "nanCountTemp = dfDemogr.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dfDemogr.columns]).toPandas()\n",
    "nanCount = pd.melt(nanCountTemp, var_name='cols', value_name='values')\n",
    "\n",
    "nanCount['missing_values_percent'] = nanCount['values']/total*100\n",
    "\n",
    "nanCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfTemp has 0 duplucates.\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate entries\n",
    "countAll = dfDemogr.count()\n",
    "\n",
    "dfDemogrT = dfDemogr.dropDuplicates(subset=['City', 'State', 'State Code', 'Race'])\n",
    "countWithDupl = dfDemogrT.count()\n",
    "\n",
    "print(\"dfTemp has \" + str(countAll - countWithDupl) + \" duplucates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The demographics dataset will provide the dimension table with information on different U.S. states. We will create a table containing one row for each state. We will do a pivoting for the race values to have separate columns for each race. The NaN-Values in all the columns will be replaces with zeroes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "![data model](udacity_capstone.jpg \"Data Model\" )\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The pipeline steps are as follows:\n",
    "\n",
    "1. Transfer the data from the local directory to S3\n",
    "- etl_immigration: load immigration data from local repository, create immigration dataframe and date dataframe, load both dataframes to an S3 bucket as parquet-files\n",
    "- etl_countries: load temperature data from local repository, create countries dataframe and load the dataframe to an S3 bucket as parquet-files\n",
    "- etl_states: load demographics data from local repository, create states dataframe and load the dataframe to an S3 bucket as parquet-files\n",
    "2. Transfer the data to a data warehouse\n",
    "- create fact and dimension tables in Amazon Redshift respectively\n",
    "- load staging data from S3 to Redshift via Airflow\n",
    "- data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**From immigration data create immigration and date dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# udf needed for immigration etl\n",
    "def date_diff(date1, date2):\n",
    "    '''\n",
    "    Calculates the difference in days between two dates\n",
    "    '''\n",
    "    \n",
    "    if date2 is None:\n",
    "        return None\n",
    "    else:\n",
    "        a = datetime.strptime(date1, \"%Y-%m-%d\")\n",
    "        b = datetime.strptime(date2, \"%Y-%m-%d\")\n",
    "        delta = b - a\n",
    "        return delta.days\n",
    "\n",
    "\n",
    "\n",
    "# User defined functions using Spark udf wrapper function for date operations\n",
    "convert_sas_udf = udf(lambda x: x if x is None else (datetime(1960,1,1) + timedelta(days=x)).strftime(\"%Y-%m-%d\"))\n",
    "date_diff_udf = udf(date_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def etl_immigration(spark, input_path=\"immigration_data_sample.csv\", output_path=None, date_output_path=None,\n",
    "                         input_format = \"csv\", columns = ['cicid','i94res','i94cit','arrdate','i94addr','i94yr','i94mon','i94port','i94mode','i94visa','visatype',\n",
    "                                                          'depdate','dtaddto','entdepa','entdepd','i94bir','biryear','gender','airline'],\n",
    "                                                          load_size = None, partitionBy = [\"i94yr\", \"i94mon\"], **options):    \n",
    "    \"\"\"\n",
    "    Reads the immigration dataset from the input_path, transforms it to the immigration fact table and dimension date table and saves both to the output_path\n",
    "    output_path.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): Spark session\n",
    "        input_path (string): Directory with the input files, default a data sample\n",
    "        output_path (string): Directory to save immigration output files\n",
    "        date_output_path (string): Directory to save date output files\n",
    "        input_format (str): Format of the input files, default to \"csv\"\n",
    "        columns (list): List of the columns names to read in\n",
    "        load_size (int): Number of rows to read for debug purposes\n",
    "        partitionBy (list): Files will be saved in partitions using the columns of this list\n",
    "        options: All other string options\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loads the immigration dataframe using Spark\n",
    "    # After the data analysis we choose only selected columns straightaway\n",
    "    dfImmigration = spark.read.format(input_format).load(input_path).select(columns)\n",
    "    \n",
    "    # Converts columns to integer\n",
    "    dfImmigration = dfImmigration.withColumn(\"cicid\", dfImmigration[\"cicid\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"i94res\", dfImmigration[\"i94res\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"i94cit\", dfImmigration[\"i94cit\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"arrdate\", dfImmigration[\"arrdate\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"i94yr\", dfImmigration[\"i94yr\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"i94mon\", dfImmigration[\"i94mon\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"i94mode\", dfImmigration[\"i94mode\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"i94visa\", dfImmigration[\"i94visa\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"i94bir\", dfImmigration[\"i94bir\"].cast(IntegerType()))\\\n",
    "        .withColumn(\"biryear\", dfImmigration[\"biryear\"].cast(IntegerType()))\n",
    "    \n",
    "    # Converts SAS date to a string date in the format of YYYY-MM-DD with a udf\n",
    "    dfImmigration = dfImmigration.withColumn(\"arrdate\", convert_sas_udf(dfImmigration['arrdate']))\\\n",
    "        .withColumn(\"depdate\", convert_sas_udf(dfImmigration['depdate']))\n",
    "        \n",
    "    # Creates a new column with the length of the visitor's stay in the US using udf\n",
    "    dfImmigration = dfImmigration.withColumn('stay', date_diff_udf(dfImmigration.arrdate, dfImmigration.depdate))\n",
    "    dfImmigration = dfImmigration.withColumn(\"stay\", dfImmigration[\"stay\"].cast(IntegerType()))\\\n",
    "    \n",
    "    # Generates date dataframe and saves it to the date_output_path\n",
    "    if date_output_path is not None:\n",
    "        arrdate = dfImmigration.select('arrdate').distinct()\n",
    "        depdate = dfImmigration.select('depdate').distinct()\n",
    "        dates = arrdate.union(depdate)\n",
    "        dates = dates.withColumn(\"date\", to_date(dates.arrdate))\n",
    "        dates = dates.withColumn(\"year\", year(dates.date))\n",
    "        dates = dates.withColumn(\"month\", month(dates.date))\n",
    "        dates = dates.withColumn(\"day\", dayofmonth(dates.date))\n",
    "        dates = dates.withColumn(\"weekofyear\", weekofyear(dates.date))\n",
    "        dates = dates.withColumn(\"dayofweek\", dayofweek(dates.date))\n",
    "        dates = dates.drop(\"date\").withColumnRenamed('arrdate', 'date')\n",
    "        dates.select([\"date\", \"year\", \"month\",\"day\",\"weekofyear\",\"dayofweek\"]).write.save(date_output_path,mode= \"overwrite\",format=\"parquet\")\n",
    "    \n",
    "    # Saves the immigration dataframe to the output_path\n",
    "    if output_path is not None:\n",
    "        dfImmigration.select(columns).write.save(output_path, mode=\"overwrite\", format=\"parquet\", partitionBy = partitionBy)\n",
    "    return dfImmigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**From world temperature data create the country dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def etl_countries(spark, input_path=\"../../data2/GlobalLandTemperaturesByCity.csv\", output_path=None, \n",
    "                         input_format = \"csv\", columns = '*', load_size = 1000, partitionBy = [\"CountryName\"], **options):\n",
    "    \"\"\"\n",
    "    Reads the temperature dataset from the input_path, transforms it to the country dimension table and saves it to the output_path.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): Spark session\n",
    "        input_path (string): Directory with the input files\n",
    "        output_path (string): Directory to save immigration output files\n",
    "        input_format (str): Format of the input files, default to \"csv\"\n",
    "        columns (list): List of the columns names to read in\n",
    "        load_size (int): Number of rows to read for debug purposes\n",
    "        partitionBy (list): Files will be saved in partitions using the columns of this list\n",
    "        options: All other string options\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loads the temperature dataframe using Spark\n",
    "    dfTemp = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Aggregates the dataset by country and renames the name of new columns\n",
    "    dfTempAvg = dfTemp.groupby([\"Country\"]).avg()\\\n",
    "        .select([\"Country\",\"avg(AverageTemperature)\"])\\\n",
    "        .withColumnRenamed('Country','CountryName')\\\n",
    "        .withColumnRenamed('avg(AverageTemperature)', 'Temperature')\n",
    "    \n",
    "    # Renames some country found in the data analysis to enable a join with the country code list\n",
    "    dfTempAvg = dfTempAvg.withColumn('CountryName', when(dfTempAvg['CountryName'] == 'Congo (Democratic Republic Of The', 'Congo').otherwise(dfTempAvg['CountryName']))\n",
    "    dfTempAvg = dfTempAvg.withColumn('CountryName', when(dfTempAvg['CountryName'] == \"Côte D'Ivoire\", 'Ivory Coast').otherwise(dfTempAvg['CountryName']))\n",
    "    \n",
    "    # Creates a column with the country name in all lower letters to enable the join with the country code list\n",
    "    dfTempAvg = dfTempAvg.withColumn('Country_Lower', lower(dfTempAvg.CountryName))\n",
    "    \n",
    "    # Loads the table of countries and their country code\n",
    "    i94_country = spark.read.load(\"I94_Country.csv\", format=\"csv\", sep=\";\", header=True).select(['Code','Country'])\n",
    "    \n",
    "    # Renames some country found in the data analysis to enable a join with the countries from the temperature data frame\n",
    "    i94_country = i94_country.withColumn('Country', when(i94_country['Country'] == 'BOSNIA-HERZEGOVINA', 'BOSNIA AND HERZEGOVINA').otherwise(i94_country['Country']))\n",
    "    i94_country = i94_country.withColumn('Country', when(i94_country['Country'] == \"INVALID: CANADA\", 'CANADA').otherwise(i94_country['Country']))\n",
    "    i94_country = i94_country.withColumn('Country', when(i94_country['Country'] == \"CHINA, PRC\", 'CHINA').otherwise(i94_country['Country']))\n",
    "    i94_country = i94_country.withColumn('Country', when(i94_country['Country'] == \"GUINEA-BISSAU\", 'GUINEA BISSAU').otherwise(i94_country['Country']))\n",
    "    i94_country = i94_country.withColumn('Country', when(i94_country['Country'] == \"INVALID: PUERTO RICO\", 'PUERTO RICO').otherwise(i94_country['Country']))\n",
    "    i94_country = i94_country.withColumn('Country', when(i94_country['Country'] == \"INVALID: UNITED STATES\", 'UNITED STATES').otherwise(i94_country['Country']))\n",
    "    i94_country = i94_country.withColumn('Country', when(i94_country['Country'] == \"MEXICO Air Sea, and Not Reported (I-94, no land arrivals)\", 'MEXICO').otherwise(i94_country['Country']))\n",
    "    \n",
    "    # Creates a column with the country name in all lower letters to enable the join with the country code list\n",
    "    i94_country = i94_country.withColumn('i94_Country_Lower', lower(i94_country.Country))\n",
    "    i94_country = i94_country.withColumn('Code', i94_country['Code'].cast(IntegerType()))\n",
    "    \n",
    "    # Joins both country tables to create a country dimension table\n",
    "    i94_country = i94_country.join(dfTempAvg, i94_country.i94_Country_Lower == dfTempAvg.Country_Lower, how=\"left\")\n",
    "    \n",
    "    dfCountry = i94_country.withColumn(\"CountryName1\", initcap(col(\"i94_Country_Lower\")))\\\n",
    "        .select([\"Code\",\"CountryName1\",\"Temperature\"])\\\n",
    "        .withColumnRenamed(\"CountryName1\",\"CountryName\")\n",
    "    \n",
    "    # Saves the country dataframe to the output_path\n",
    "    if output_path is not None:\n",
    "        dfCountry.select(columns).write.save(output_path, mode=\"overwrite\", format=\"parquet\", partitionBy = partitionBy)\n",
    "    return dfCountry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**From demographics data create the state dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def etl_states(spark, input_path=\"us-cities-demographics.csv\", output_path=None, \n",
    "                         input_format = \"csv\", columns='*', load_size = None, partitionBy = [\"StateCode\"], sep=\";\", **options):\n",
    "    \"\"\"\n",
    "    Reads the cities demographics dataset from the input_path, transforms it to the state dimension table and saves it to the output_path.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): Spark session\n",
    "        input_path (string): Directory with the input files\n",
    "        output_path (string): Directory to save immigration output files\n",
    "        input_format (str): Format of the input files, default to \"csv\"\n",
    "        columns (list): List of the columns names to read in\n",
    "        load_size (int): Number of rows to read for debug purposes\n",
    "        partitionBy (list): Files will be saved in partitions using the columns of this list\n",
    "        sep (string): Separator for reading the csv-file\n",
    "        options: All other string options\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loads the demografics dataframe using Spark\n",
    "    dfDemogr = spark.read.csv(input_path, inferSchema=True, header=True, sep=sep)\n",
    "    \n",
    "    # Aggregates some values to the city level: Since those values are repeated in every row with a certain city, we take the min\n",
    "    dfDemogrAggr = dfDemogr.groupby([\"City\",\"State\",\"State Code\"]).min()\\\n",
    "        .withColumnRenamed('min(Median Age)', 'MedianAge')\\\n",
    "        .withColumnRenamed('min(Average Household Size)', 'AverageHouseholdSize')\\\n",
    "        .withColumnRenamed('min(Male Population)', 'MalePopulation')\\\n",
    "        .withColumnRenamed('min(Female Population)', 'FemalePopulation')\\\n",
    "        .withColumnRenamed('min(Total Population)', 'TotalPopulation')\\\n",
    "        .withColumnRenamed('min(Number of Veterans)', 'NumberOfVeterans')\\\n",
    "        .withColumnRenamed('min(Foreign-born)', 'ForeignBorn')\\\n",
    "        .select([\"City\",\"State\",\"State Code\",\"MedianAge\",\"AverageHouseholdSize\",\"MalePopulation\",\"FemalePopulation\",\\\n",
    "                 \"TotalPopulation\",\"NumberOfVeterans\",\"ForeignBorn\"])\n",
    "    \n",
    "    # Creates a pivot for race to aggregate the numbers to one row for each city\n",
    "    dfDemogrPiv = dfDemogr.groupBy([\"City\", \"State\", \"State Code\"]).pivot(\"Race\").sum(\"Count\")\\\n",
    "        .withColumnRenamed('American Indian and Alaska Native', 'AmericanIndianAndAlaskaNative')\\\n",
    "        .withColumnRenamed('Black or African-American', 'BlackOrAfricanAmerican')\\\n",
    "        .withColumnRenamed('Hispanic or Latino', 'HispanicOrLatino')\n",
    "    \n",
    "    # Joins both tables\n",
    "    dfDemogrF = dfDemogrAggr.join(other=dfDemogrPiv, on=[\"City\", \"State\", \"State Code\"], how=\"inner\")\\\n",
    "        .withColumnRenamed('State Code', 'StateCode')\n",
    "    \n",
    "    # Replaces missing values with 0\n",
    "    dfDemogrF = dfDemogrF.fillna(0, ['MedianAge', 'AverageHouseholdSize', 'MalePopulation', 'FemalePopulation', 'TotalPopulation', 'NumberOfVeterans', 'ForeignBorn',\n",
    "                                     'AmericanIndianAndAlaskaNative', 'Asian', 'BlackOrAfricanAmerican', 'HispanicOrLatino', 'White'])\n",
    "    \n",
    "    # Converts columns to integer\n",
    "    dfDemogrF = dfDemogrF.withColumn('MalePopulation', dfDemogrF['MalePopulation'].cast(IntegerType()))\\\n",
    "        .withColumn('FemalePopulation', dfDemogrF['FemalePopulation'].cast(IntegerType()))\\\n",
    "        .withColumn('TotalPopulation', dfDemogrF['TotalPopulation'].cast(IntegerType()))\\\n",
    "        .withColumn('NumberOfVeterans', dfDemogrF['NumberOfVeterans'].cast(IntegerType()))\\\n",
    "        .withColumn('ForeignBorn', dfDemogrF['ForeignBorn'].cast(IntegerType()))\\\n",
    "        .withColumn('AmericanIndianAndAlaskaNative', dfDemogrF['AmericanIndianAndAlaskaNative'].cast(IntegerType()))\\\n",
    "        .withColumn('Asian', dfDemogrF['Asian'].cast(IntegerType()))\\\n",
    "        .withColumn('BlackOrAfricanAmerican', dfDemogrF['BlackOrAfricanAmerican'].cast(IntegerType()))\\\n",
    "        .withColumn('HispanicOrLatino', dfDemogrF['HispanicOrLatino'].cast(IntegerType()))\\\n",
    "        .withColumn('White', dfDemogrF['White'].cast(IntegerType()))\n",
    "    \n",
    "    # To aggregate the columns to the state level we have to recalculate the average considering number of total population in every city\n",
    "    dfDemogrAvg = dfDemogrF.withColumn(\"MedianAgeSum\", dfDemogrF.MedianAge * dfDemogrF.TotalPopulation)\\\n",
    "        .withColumn(\"AverageHouseholdSizeSum\", dfDemogrF.AverageHouseholdSize * dfDemogrF.TotalPopulation)  \n",
    "    dfDemogrSum = dfDemogrAvg.groupby([\"State\",\"StateCode\"]).sum()\\\n",
    "        .select([\"State\",\"StateCode\",\"sum(TotalPopulation)\",\"sum(MalePopulation)\",\"sum(FemalePopulation)\",\"sum(ForeignBorn)\",\"sum(NumberOfVeterans)\",\n",
    "                 \"sum(AmericanIndianAndAlaskaNative)\",\"sum(Asian)\",\"sum(BlackOrAfricanAmerican)\",\"sum(HispanicOrLatino)\",\"sum(White)\",\n",
    "                 \"sum(MedianAgeSum)\",\"sum(AverageHouseholdSizeSum)\"])\n",
    "    dfDemogrState = dfDemogrSum.withColumn(\"MedianAgeNew\", (dfDemogrSum[\"sum(MedianAgeSum)\"]/dfDemogrSum[\"sum(TotalPopulation)\"]).cast(DoubleType()))\\\n",
    "        .withColumn(\"AverageHouseholdSizeNew\", (dfDemogrSum[\"sum(AverageHouseholdSizeSum)\"]/dfDemogrSum[\"sum(TotalPopulation)\"]).cast(DoubleType()))\n",
    "    \n",
    "    # Renames the columns\n",
    "    dfDemogrState = dfDemogrState.withColumnRenamed(\"sum(MalePopulation)\", \"MalePopulation\")\\\n",
    "        .withColumnRenamed(\"sum(FemalePopulation)\", \"FemalePopulation\")\\\n",
    "        .withColumnRenamed(\"sum(TotalPopulation)\", \"TotalPopulation\")\\\n",
    "        .withColumnRenamed(\"sum(ForeignBorn)\", \"ForeignBorn\")\\\n",
    "        .withColumnRenamed(\"sum(NumberOfVeterans)\", \"NumberOfVeterans\")\\\n",
    "        .withColumnRenamed(\"sum(AmericanIndianAndAlaskaNative)\", \"AmericanIndianAndAlaskaNative\")\\\n",
    "        .withColumnRenamed(\"sum(Asian)\", \"Asian\")\\\n",
    "        .withColumnRenamed(\"sum(BlackOrAfricanAmerican)\", \"BlackOrAfricanAmerican\")\\\n",
    "        .withColumnRenamed(\"sum(HispanicOrLatino)\", \"HispanicOrLatino\")\\\n",
    "        .withColumnRenamed(\"sum(White)\", \"White\")\n",
    "    \n",
    "    dfState = dfDemogrState.drop(\"sum(MedianAgeSum)\",\"sum(AverageHouseholdSizeSum)\")\\\n",
    "        .withColumnRenamed(\"MedianAgeNew\", \"MedianAge\")\\\n",
    "        .withColumnRenamed(\"AverageHouseholdSizeNew\", \"AverageHouseholdSize\")\n",
    "    \n",
    "    # Saves the state dataframe to the output_path\n",
    "    if output_path is not None:\n",
    "        dfState.select(columns).write.save(output_path, mode=\"overwrite\", format=\"parquet\", partitionBy = partitionBy)\n",
    "    return dfState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Given the output_path directories, the etl-functions perform the first step of our data pipeline - transfer the data from local directories to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    # Perform ETL process to the immigration data and load it into s3 bucket\n",
    "    immigration = etl_immigration(spark, input_path='../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat',\n",
    "                                     output_path=output_data+\"immigration.parquet\",\n",
    "                                     date_output_path=output_data+\"date.parquet\",\n",
    "                                     input_format = \"com.github.saurfang.sas.spark\", \n",
    "                                     load_size=1000, partitionBy=None)\n",
    "    \n",
    "    # Perform ETL process to the immigration data and load it into s3 bucket\n",
    "    countries = etl_countries(spark, output_path=output_data + \"country.parquet\")\n",
    "    \n",
    "    # Perform ETL process to the immigration data and load it into s3 bucket\n",
    "    states = etl_states(spark, output_path=output_data + \"state.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "With Apache Airflow we now can insert the data into created tables on Amazon Redshift. The quality check takes place in the Airflow routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Unfortunately I cannot upload a picture of the airflow tree or execution plan since the stupid workspace shows an error today which didn't exist yesterday and I honestly don't have anymore nerves for this stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data check is performed within the Airflow dag\n",
    "def execute(self, context):\n",
    "    redshift_hook = PostgresHook(self.redshift_conn_id)\n",
    "\n",
    "    for table in self.tables:\n",
    "        records = redshift_hook.get_records(f\"SELECT COUNT(*) FROM {table}\")\n",
    "        if len(records) < 1 or len(records[0]) < 1:\n",
    "            raise ValueError(f\"Data quality check failed. {table} returned no results\")\n",
    "        num_records = records[0][0]\n",
    "        if num_records < 1:\n",
    "            raise ValueError(f\"Data quality check failed. {table} contained 0 rows\")\n",
    "\n",
    "        self.log.info(f\"Data quality on table {table} check passed with {records[0][0]} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "*Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.*\n",
    "\n",
    "Data dictionary was already repeated throughtout the whole project multiple times but I copy and paste it again with pleasure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Immigration face table:\n",
    "\n",
    "|Column Name|Description|\n",
    "|---|---|\n",
    "|_**cicid**_|Record ID|\n",
    "|**i94yr**|Year of arrival, 4 digits|\n",
    "|**i94mon**|Month of arrival, numeric|\n",
    "|**i94cit**|Citizenship country code|\n",
    "|_**i94res**_|Residence country code|\n",
    "|**i94port**|U.S. port of entry (main gateway ports - all modes and air-only)|\n",
    "|_**arrdate**_|Arrival date in the USA|\n",
    "|**i94mode**|Mode of transportation (1=air, 2=sea, 3=land, 9=not reported)|\n",
    "|_**i94addr**_|First Intended Address in the U.S. (or Address while in the U.S.) (state)|\n",
    "|**depdate**|Departure date from the USA|\n",
    "|**i94bir**|Age of Respondent in Years|\n",
    "|**i94visa**|Visa code (1=business, 2=pleasure, 3=student)|\n",
    "|**visapost**|Department of State where where Visa was issued|\n",
    "|**entdepa**|Arrival Flag - admitted or paroled into the U.S.|\n",
    "|**entdepd**|Departure Flag - Departed, lost I-94 or is deceased|\n",
    "|**biryear**|Birth year, 4 digits|\n",
    "|**dtaddto**|Character Date Field - Date to which admitted to U.S. (allowed to stay until)|\n",
    "|**gender**|Gender|\n",
    "|**airline**|Airline used to arrive in U.S.|\n",
    "|**visatype**|Class of admission legally admitting the non-immigrant to temporarily stay in U.S.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Country dimension table:\n",
    "\n",
    "|Column Name|Description|\n",
    "|---|---|\n",
    "|CountryCode|Country code|\n",
    "|Country|Country name|\n",
    "|AverageTemperature|Average land temperature of the country in Celsius|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### State dimension dataset:\n",
    "\n",
    "|Column Name|Description|\n",
    "|---|---|\n",
    "|StateCode|State Code|\n",
    "|State|US state|\n",
    "|Total Population|Number of the total population|\n",
    "|Male Population|Number of males|\n",
    "|Female Population|Number of females|\n",
    "|Number Veterans|Number of veterans|\n",
    "|Foreign-born|Number of residents that born outside th United States|\n",
    "|Median Age|Median of the age of the population|\n",
    "|Average Household Size|Average size of the household|\n",
    "|AmericanIndianAndAlaskaNative|Number of residents of the race American Indian And Alaska Native|\n",
    "|Asian|Number of residents of the race Asian|\n",
    "|BlackOrAfricanAmerican|Number of residents of the race Black Or African American|\n",
    "|HispanicOrLatino|Number of residents of the race Hispanic Or Latino|\n",
    "|White|Number of residents of the race White|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Date dimension table:\n",
    "\n",
    "|Column Name|Description|\n",
    "|---|---|\n",
    "|date|Date in the format YYYY-MM-DD|\n",
    "|year|Four digit year|\n",
    "|month|Two digit month|\n",
    "|day|Two digit day|\n",
    "|weekofyear|The week of the year|\n",
    "|dayofweek|The day of the week|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "*Clearly state the rationale for the choice of tools and technologies for the project.*\n",
    "* Aparche Spark was used because of its speed with large scale data processing, its ability to hangle multiple file formats. It also comes packaged with higher-level libraries, including support for SQL queries, and much more\n",
    "* Amazon S3 was used because it provides a relatively cheap, easy-to-use with scalability, high availability, security, and performance solution for the staging of big data\n",
    "* Since we were using AWS products, we also used Redshift - a parallel, column-oriented data warehouse with easy-scale functionality\n",
    "\n",
    "*Propose how often the data should be updated and why.*\n",
    "* The data should be updated monthly respectively to the I94 Immigration data update.\n",
    "\n",
    "*Write a description of how you would approach the problem differently under the following scenarios:*\n",
    "1. The data was increased by 100x.\n",
    "* Increasing of the data should not be a problem for Spark. We could consider increasing the number of nodes we are using in our EMR cluster of AWS and we could use a bigger node for Amazon Redshift. Both solutions are provided by AWS.\n",
    " \n",
    "2. The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "* Airflow should run daily before 7am to provide necessary data for the dashboard. The fact table is only updated monthly, but depending on the dashboard aggregations it makes sence to schedule a daily update.\n",
    "\n",
    "3. The database needed to be accessed by 100+ people.\n",
    "* With Amazon Redshift we can quickly add or remove nodes. This allows us to get better performance and more storage for demanding workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
